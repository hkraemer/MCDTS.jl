var documenterSearchIndex = {"docs":
[{"location":"ref/#Reference","page":"Reference","title":"Reference","text":"","category":"section"},{"location":"ref/","page":"Reference","title":"Reference","text":"Modules = [MCDTS]\nOrder   = [:type, :function]","category":"page"},{"location":"ref/#MCDTS.AbstractTreeElement","page":"Reference","title":"MCDTS.AbstractTreeElement","text":"The MCDTS algorithm is implemented as a tree with different kind types encoding\nthe leafs and the root of the tree. AbstractTreeElement is the abstract type of\nthese types.\n\n\n\n\n\n","category":"type"},{"location":"ref/#MCDTS.CCM_ρ","page":"Reference","title":"MCDTS.CCM_ρ","text":"CCM_ρ <: AbstractLoss\n\nConstructor for the CCM_ρ loss function (correlation coefficient of the\nconvergent cross mapping) based on Sugihara et al.[^Sugihara2012], see also\n[`ccm`](@ref). In this case MCDTS tries to maximize the correlation coefficient\nof the convergent cross mapping from the input `data` and `Y_CCM`, the time\nseries CCM should cross map to from `data` (see [`mcdts_embedding`](@ref)).\n\n## Fieldnames\n* `timeseries`: The time series CCM should cross map to.\n* `threshold::Float`: A threshold for the sufficient correlation of the\n  cross-mapped values and the true values from `Y_CMM` for the current embedding.\n  When the correlation coefficient exeeds this threshold in an embedding cycle\n  the embedding stops.\n\n## Defaults\n* When calling `CCM_ρ(timeseries)`, a CCM_ρ-object is created, storing\n  `timeseries`, which is considered to be causally depended and the\n  `threshold=1` is used, i.e. no threshold, since the correlation coefficient\n  can not exceed 1.\n\n[^Sugihara2012]: Sugihara et al., [Detecting Causality in Complex Ecosystems. Science 338, 6106, 496-500](https://doi.org/10.1126/science.1227079)\n\n\n\n\n\n","category":"type"},{"location":"ref/#MCDTS.Continuity_function","page":"Reference","title":"MCDTS.Continuity_function","text":"Continuity_function <: AbstractDelayPreselection\n\nConstructor for the continuity function `⟨ε★⟩` by Pecora et al.[^Pecora2007],\nsee [`pecora`](@ref).\n\n## Fieldnames\n* `K::Int`: the amount of nearest neighbors in the δ-ball. Must be at\n  least 8 (in order to gurantee a valid statistic). `⟨ε★⟩` is computed taking\n  the minimum result over all `k ∈ K` (read algorithm description in [`pecora`](@ref)).\n* `samplesize::Real`: determine the fraction of all phase space points\n  to be considered (fiducial points v) to average ε★ to produce `⟨ε★⟩`.\n* `α::Real = 0.05`: The significance level for obtaining the continuity statistic\n* `p::Real = 0.5`: The p-parameter for the binomial distribution used for the\n  computation of the continuity statistic ⟨ε★⟩.\n\n## Defaults\n* When calling `Continuity_function()` a Continuity_function-object is constructed\n  with `K=13`, `samplesize=1.`, `α=0.05` and `p=0.5`.\n\n\n\n\n\n","category":"type"},{"location":"ref/#MCDTS.FNN_statistic","page":"Reference","title":"MCDTS.FNN_statistic","text":"FNN_statistic <: AbstractLoss\n\nConstructor for the FNN-statistic loss function (false nearest neighbor) based\non Hegger & Kantz [^Hegger1999].\n\n## Fieldnames\n* `threshold::Float`: A threshold for the tolerable cumulative FNN decrease\n  for the current embedding. When the fraction of FNNs fall below this threshold\n  in an embedding cycle the embedding stops.\n* `r::Float`: The FNN-distance-expansion threshold (typically set to 2).\n\n## Defaults\n* When calling `FNN_statistic()`, a FNN_statistic-object is created, which uses no\n  threshold and uses the FNN-inter threshold `r=2`.\n* When calling `FNN_statistic(threshold)`, a FNN_statistic-object is created, which uses\n  the given `threshold` and uses the FNN-inter threshold `r=2`.\n\n[^Hegger1999]: Hegger & Kantz, [Improved false nearest neighbor method to detect determinism in time series data. Physical Review E 60, 4970](https://doi.org/10.1103/PhysRevE.60.4970).\n\n\n\n\n\n","category":"type"},{"location":"ref/#MCDTS.L_statistic","page":"Reference","title":"MCDTS.L_statistic","text":"L_statistic <: AbstractLoss\n\nConstructor for the L-statistic loss function based on Uzal et al.[^Uzal2011]. Here\nwe consider the decrease of the L-statistic `ΔL` in between embedding cycles,\naccording to Kraemer et al.[^Kraemer2021] ([`pecuzal_embedding`](@ref)).\n\n## Fieldnames\n* `threshold::Float`: A threshold for the tolerable `ΔL` decrease for the current\n  embedding. When `ΔL` exceeds this threshold in an embedding cycle the embedding\n  stops. Note that `ΔL` is a negative value therefore `threshold` must be a small\n  negative number.\n* `KNN::Int`: the amount of nearest neighbors considered, in order to compute the\n L-statistic, in particular `σ_k^2` (read algorithm description [`uzal_cost`]@ref).\n* `tws::AbstractRange{Int}`: Customization of the sampling of the different time horizons\n  (T's), when computing Uzal's L-statistics. Here any kind of integer ranges (starting at 2)\n  are allowed.\n\n## Defaults\n* When calling `L_statistic()`, a L_statistic-object is created, which uses no\n  threshold and consideres 3 nearest neighbors for time horizons `tws=2:100`.\n* When calling `L_statistic(threshold)`, a L_statistic-object is created, which uses\n  the given `threshold` and consideres 3 nearest neighbors for time horizons `tws=2:100`.\n* When calling `L_statistic(threshold,KNN)`, a L_statistic-object is created, which uses\n  the given `threshold`, consideres `KNN` nearest neighbors for time horizons `tws=2:100`.\n\n[^Kraemer2021]: Kraemer, K.H., Datseris, G., Kurths, J., Kiss, I.Z., Ocampo-Espindola, Marwan, N. (2021). [A unified and automated approach to attractor reconstruction. New Journal of Physics 23(3), 033017](https://iopscience.iop.org/article/10.1088/1367-2630/abe336).\n\n[^Uzal2011]: Uzal, L. C., Grinblat, G. L., Verdes, P. F. (2011). [Optimal reconstruction of dynamical systems: A noise amplification approach. Physical Review E 84, 016223](https://doi.org/10.1103/PhysRevE.84.016223).\n\n\n\n\n\n","category":"type"},{"location":"ref/#MCDTS.MCDTSOptimGoal","page":"Reference","title":"MCDTS.MCDTSOptimGoal","text":"MCDTSOptimGoal <: AbstractMCDTSOptimGoal\n\nConstructor, which handles the loss-/objective function `Γ` and the delay\npre-selection statistic `Λ` MCDTS uses.\n\n## Fieldnames\n* `Γ::AbstractLoss`: Chosen loss-function, see the so far available\n  [`L_statistic`](@ref), [`FNN_statistic`](@ref), [`CCM_ρ`](@ref) and\n  [`Prediction_error`](@ref).\n* `Λ::AbstractDelayPreselection`: Chosen delay Pre-selection method, see the so\n  far available [`Continuity_function`](@ref) and [`Range_function`](@ref).\n\n## Defaults\n* TBD\n\n\n\n\n\n","category":"type"},{"location":"ref/#MCDTS.MCDTSpredictionType","page":"Reference","title":"MCDTS.MCDTSpredictionType","text":"MCDTSpredictionType <: AbstractMCDTSpredictionType\n\nConstructor, which determines the way how predictions are made technically.\n\n## Fieldnames\n* `loss::AbstractPredictionLoss`: Indicates the way of computing the prediction error.\n   See [`PredictionLoss`](@ref) for information on how to construct this object.\n* `method::AbstractPredictionMethod`: The method based on the state space reconstruction,\n   which makes the actual prediction. See [`local_model`](@ref)\n\n## Default settings\n* When calling `MCDTSpredictionType()` a MCDTSpredictionType-object is constructed\n  with a `local_zeroth`-predictor [`local_model`](@ref), using 2 nearest neighbors\n  and a 1-step-ahead-prediction. The loss-function is the root mean squared prediction\n  error over all components [`PredictionLoss`](@ref).\n\n\n\n\n\n","category":"type"},{"location":"ref/#MCDTS.Node","page":"Reference","title":"MCDTS.Node","text":"mutable struct Node{T}\n\nA node of the tree. Each node contains its children and information about the current embedding.\n\n## Fieldnames:\n* `τ::Int`: The delay value of this node\n* `L::T`: The value of the cumulative ΔL statistic at this node\n* `τs::Array{Int,1}`: The complete vector with all τs chosen along this path up until this node\n* `ts::Array{Int,1}`: The complex vector which of the possibly multivariate time series is used at each embedding step i\n* `children::Union{Array{Node,1},Nothing}`: The children of this node\n* `temp::S`: additional \"work\"/\"temporary\" array/field that can be manipulated and saved by the optimization e.g. to reuse prior computations\n\n\n\n\n\n","category":"type"},{"location":"ref/#MCDTS.PredictionLoss","page":"Reference","title":"MCDTS.PredictionLoss","text":"PredictionLoss <: AbstractPredictionLoss\n\nConstructor, which indicates the way of computing the prediction error. This\nobject is used for the constructor, which determines the way how predictions are\nmade methodologically [`MCDTSpredictionType`](@ref).\n\n## Fieldnames\n* `type::Int` is an integer, which encodes the type of prediction error:\n* For `type = 1` the root mean squared prediction error over the first component,\n  i.e. the timeseries, which needs to be predicted, is used. (default)\n* For `type = 2` the root mean squared prediction error over all components\n  (dimensionality of the state space) is used.\n* For `type = 3` the mean Kullback-Leibler Distance of the predicted and the true\n  values of the first component, i.e. the timeseries, which needs to be predicted, is used.\n* For `type = 4` the mean Kullback-Leibler Distance of the predicted and the true\n  values over all components (dimensionality of the state space) is used.\n\n## Default settings\n* When calling `PredictionLoss()` a PredictionLoss-object is constructed with\n  fieldname `type = 1` (≡root mean squared prediction error over all components)\n\n\n\n\n\n","category":"type"},{"location":"ref/#MCDTS.Prediction_error","page":"Reference","title":"MCDTS.Prediction_error","text":"Prediction_error <: AbstractLoss\n\nConstructor for the Prediction_error loss function.\n\n## Fieldnames\n* `PredictionType::MCDTSpredictionType`: Determines the prediction type by\n  setting a prediction-method and the way the prediction error is measured,\n  see [`MCDTSpredictionType`](@ref).\n* `threshold::Float`: A threshold for the sufficient minimum prediction error\n  for the current embedding. When the prediction error, specified in\n  `PredictionType`, falls below this threshold in an embedding cycle the\n  embedding stops.\n\n## Defaults\n* When calling `Prediction_error()`, a Prediction_error-object is created,\n  which uses the threshold 0, i.e. no threshold and a zeroth-order predictor\n  (see [`MCDTSpredictionType`](@ref), [`PredictionLoss`](@ref) &\n  [`local_model`](@ref))\n\n\n\n\n\n","category":"type"},{"location":"ref/#MCDTS.Range_function","page":"Reference","title":"MCDTS.Range_function","text":"Range_function <: AbstractDelayPreselection\n\nConstructor for a range of possible delay values. In this case there is\nactually no \"pre-selection\" of delay, but rather all possible delays, given\nin the input `τs` (see, [`mcdts_embedding`](@ref)) are considered. This can significantly affect the\ncomputation time. There are no fieldnames, simply construct by typing\n`RangeFunction()`.\n\n\n\n\n\n","category":"type"},{"location":"ref/#MCDTS.Root","page":"Reference","title":"MCDTS.Root","text":"mutable struct Root <: AbstractTreeElement\n\nThe 'start'/root of Tree. Each node contains its children. The root contains the starting branches/nodes.\nFor initialization type `r = Root()`.\n\n## Fieldnames:\n* `children::Union{Array{Node,1},Nothing}`: The first nodes of the tree.\n* `Lmin`; Is the global minimum of the cumulative ΔL statistic found so far.\n\n\n\n\n\n","category":"type"},{"location":"ref/#MCDTS.local_model","page":"Reference","title":"MCDTS.local_model","text":"local_model <: AbstractPredictionMethod\n\nConstructor, which indicates the local state space prediction model.\n\n## Fieldnames\n* `method::String`: Could be `\"zeroth\"` (averaged `Tw`-step-ahead image of the\n `KNN`-nearest neighbors) or `\"linear\"` (local linear regression on the\n `KNN`-nearest neighbors).\n* `KNN::Int`: The number of considered nearest neighbors.\n* `Tw::Int` : The prediction horizon in sampling units.\n\n## Default settings\n* When calling `local_model()` a local_model-object is constructed with a zeroth\n  order prediction scheme, 2 nearest neighbors and a 1-step-ahead prediction.\n* When calling `local_model(method)` a local_model-object is constructed with a\n  `method`-prediction scheme, 2 nearest neighbors and a 1-step-ahead prediction.\n* When calling `local_model(method,KNN)` a local_model-object is constructed with a\n `method`-prediction scheme, `KNN` nearest neighbors and a 1-step-ahead prediction.\n\n\n\n\n\n","category":"type"},{"location":"ref/#MCDTS.all_neighbors","page":"Reference","title":"MCDTS.all_neighbors","text":"all_neighbors(A::Dataset, stype, w = 0) → idxs, dists\n\nFind the neighbors of all points in A using search type stype (either NeighborNumber or WithinRange) and w the Theiler window.\n\nThis function is nothing more than a convinience call to Neighborhood.bulksearch.\n\n\n\n\n\n","category":"function"},{"location":"ref/#MCDTS.ar_process-Union{Tuple{T}, Tuple{T, T, T, Int64}} where T<:Real","page":"Reference","title":"MCDTS.ar_process","text":"Generate data from a AR(1) process for a initial value `u0`, a AR-coefficient\n`α` and a white noise scaling parameter `p`. Return a time series of length `N`.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.backprop!-Tuple{MCDTS.Root, Any, Any, Any}","page":"Reference","title":"MCDTS.backprop!","text":"backprop!(n::Root,τs,ts,L_min)\n\nBackpropagation of the tree spanned by all children in `n` (for this run).\nAll children-nodes L-values get set to the final value achieved in this run.\nThis function is ususally called be [`expand!`](@ref).\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.best_embedding-Tuple{MCDTS.Root}","page":"Reference","title":"MCDTS.best_embedding","text":"best_embedding(r::Root)\n\nGiven the root `r` of a tree, return the best embedding in the form of the\nfinal node at the end of the best embedding.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.ccm-Union{Tuple{T}, Tuple{D}, Tuple{DelayEmbeddings.Dataset{D, T}, Vector{T}}} where {D, T<:Real}","page":"Reference","title":"MCDTS.ccm","text":"ccm(X, y; kwargs...) → ρ, y_hat\n\nCompute the convergent crossmapping (CCM) (Sugihara et al. 2012) of a\nvector time series `X` (an embedded time series `x`) on the time series `y`\nNOTE: 'X' and 'y' must have the same length and you have to make sure that\n'y' starts at the same time index as 'X' does. - When using [`genembed`](@ref)\nwith negative delays to construct `X` from `x`, which is mandatory here, then\n'y' needs to be shifted by the largest negative delay value, which has been\nused to construct `X`.\n\nReturns the correlation coefficient of `y` and its predicted values for `y_hat`,\nbased on the nearest neighbour structure of `X`.\nIt is said that 'y' causes 'x', if ρ increases with increasing time series\nlength AND ρ is \"quite high\".\n\nKeyword arguments:\n*`metric = Euclidean()`: The metric for vector distance computation.\n*`w::Int = 1`: The Theiler window in sampling units.\n*`lags::Array = [0]`: The lag for the cross mapping, in order to detect time lagged\n                      causal relationships. The output ρ is an array of size\n                      `length(lags)`, the output Y_hat is the one corresponding\n                      to a lag of zero.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.choose_children-Tuple{MCDTS.AbstractTreeElement, Int64, Int64}","page":"Reference","title":"MCDTS.choose_children","text":"choose_children(n::AbstractTreeElement, τ::Int, t:Int)\n\nPick one of the children of the tree node n with values τ and t. If there is none, return nothing.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.choose_next_node","page":"Reference","title":"MCDTS.choose_next_node","text":"choose_next_node(n::Union{Node,Root}, func, Lmin_global, i_trial::Int=1)\n\nReturns one of the children of based on the function `func(Ls)->i_node`,\nLmin_global is the best L value so far in the optimization process, if any of\nthe input Ls to choose from is smaller than it, it is always chosen.\n`choose_mode` is only relevant for the first embedding step right now: it\ndetermines if the first step is chosen uniform (`choose_mode==0`) or with the\n`func` (`choose_mode==1`).\n\n\n\n\n\n","category":"function"},{"location":"ref/#MCDTS.columns","page":"Reference","title":"MCDTS.columns","text":"columns(dataset) -> x, y, z, ...\n\nReturn the individual columns of the dataset.\n\n\n\n\n\n","category":"function"},{"location":"ref/#MCDTS.compute_KL_divergence-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}}} where T","page":"Reference","title":"MCDTS.compute_KL_divergence","text":"Compute the Kullback-Leibler-Divergence of the two Vectors `a` and `b`.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.compute_abs_err-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}}} where T","page":"Reference","title":"MCDTS.compute_abs_err","text":"Compute the total absolute error between `prediction` and `reference`\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.compute_costs_from_prediction-Union{Tuple{T}, Tuple{D}, Tuple{MCDTS.AbstractPredictionLoss{1}, DelayEmbeddings.AbstractDataset{D, T}, DelayEmbeddings.AbstractDataset{D, T}, Int64}} where {D, T}","page":"Reference","title":"MCDTS.compute_costs_from_prediction","text":"Compute the in-sample prediction costs based on the loss-metric determined\nby PredictionLoss\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.compute_delta_L-Union{Tuple{T}, Tuple{Vector{T}, Vector{Int64}, Int64}} where T","page":"Reference","title":"MCDTS.compute_delta_L","text":"compute_delta_L(s, τs, (js,) T_max; KNN = 3, w = 1, metric = Euclidean) → ΔL\n\nCompute the overall L-decrease `ΔL` of a given embedding of the time series\n`s::Vector` with the delay values `τs` up to a maximum `T`-value `T_max`. We\nrespect the Theiler window `w`, the chosen `metric` and the number of considered\nnearest neighbors `KNN`. It is also possible to compute `ΔL` for a multivariate\ninput `Y::Dataset`. Then one additionally needs to supply a vector `js`, which\nlists the chosen time series corresponding to the given delay values in `τs`.\nThis is similar to the procedure in [`genembed`]@ref. The computations are based\non z-standardized input for ensuring comparability.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.compute_loss","page":"Reference","title":"MCDTS.compute_loss","text":"Compute the loss of a given delay-preselection statistic `dps` and the loss\ndetermined by `optimalg.Γ`.\n\n\n\n\n\n","category":"function"},{"location":"ref/#MCDTS.compute_loss-Union{Tuple{T}, Tuple{D}, Tuple{P}, Tuple{MCDTS.CCM_ρ, MCDTS.AbstractDelayPreselection, Vector{P}, DelayEmbeddings.Dataset{D, T}, Any, Any, Int64, Int64, Any, Any}} where {P, D, T}","page":"Reference","title":"MCDTS.compute_loss","text":"Return the loss based on the negative correlation coefficient for CCM.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.compute_loss-Union{Tuple{T}, Tuple{D}, Tuple{P}, Tuple{MCDTS.FNN_statistic, MCDTS.AbstractDelayPreselection, Vector{P}, DelayEmbeddings.Dataset{D, T}, Any, Any, Int64, Int64, Any, Any}} where {P, D, T}","page":"Reference","title":"MCDTS.compute_loss","text":"Return the loss based on the FNN-statistic `FNN` and indices `max_idx`  for all local maxima in dps\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.compute_loss-Union{Tuple{T}, Tuple{D}, Tuple{P}, Tuple{MCDTS.L_statistic, MCDTS.AbstractDelayPreselection, Vector{P}, DelayEmbeddings.Dataset{D, T}, Any, Any, Int64, Int64, Any, Any}} where {P, D, T}","page":"Reference","title":"MCDTS.compute_loss","text":"Return the loss based on the maximum decrease of the L-statistic `L_decrease` and corresponding\ndelay-indices `max_idx` for all local maxima in ε★\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.compute_loss-Union{Tuple{T}, Tuple{D}, Tuple{P}, Tuple{MCDTS.Prediction_error, MCDTS.AbstractDelayPreselection, Vector{P}, DelayEmbeddings.Dataset{D, T}, Any, Any, Int64, Int64, Any, Any}} where {P, D, T}","page":"Reference","title":"MCDTS.compute_loss","text":"Return the loss based on a `Tw`-step-ahead local-prediction.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.compute_mse-Union{Tuple{T}, Tuple{Vector{T}, Vector{T}}} where T","page":"Reference","title":"MCDTS.compute_mse","text":"Compute the mean squared error between `prediction` and `reference`\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.difference_distribution-Tuple{StatsBase.Histogram, Any}","page":"Reference","title":"MCDTS.difference_distribution","text":"calculate σ(ζ) = ∫ρ(x) ρ(ζ-x) dx , approximating ζ to be in bin start (whole bin counts).\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.embed_for_prediction-Union{Tuple{T}, Tuple{D}, Tuple{DelayEmbeddings.Dataset{D, T}, Vector{T}, Int64}} where {D, T}","page":"Reference","title":"MCDTS.embed_for_prediction","text":"embed_for_prediction(Y::Dataset, x::Vector, τ::Int) → Y_embed\n\nEmbeds the trajectory (or Vector) `Y` in an additional dimension, using the time\nseries `x` and the provided lag `τ`. Here we enforce a causal embedding, meaning\nthat we shift `x` by the negative values `τ`.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.embedding_cycle-Tuple{MCDTS.AbstractMCDTSOptimGoal, Any, Any, Any, Any, Any, Any}","page":"Reference","title":"MCDTS.embedding_cycle","text":"Perform a potential embedding cycle from the multi- or univariate Dataset `Ys`.\nReturn the possible delays `τ_pot`, the associated time series `ts_pot` and\nthe corresponding L-statistic-values, `L_pot` for each peak, i.e. for each\n(`τ_pot`, `ts_pot`) pair. If `FNN=true`, `L_pot` stores the corresponding\nfnn-statistic-values.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.expand!-Union{Tuple{T}, Tuple{DT}, Tuple{D}, Tuple{MCDTS.Root, MCDTS.AbstractMCDTSOptimGoal, DelayEmbeddings.Dataset{D, T}, Int64}, Tuple{MCDTS.Root, MCDTS.AbstractMCDTSOptimGoal, DelayEmbeddings.Dataset{D, T}, Int64, AbstractRange{DT}}, Tuple{MCDTS.Root, MCDTS.AbstractMCDTSOptimGoal, DelayEmbeddings.Dataset{D, T}, Int64, AbstractRange{DT}, Int64}} where {D, DT, T<:Real}","page":"Reference","title":"MCDTS.expand!","text":"expand!(n::Union{Node,Root}, optimalg::AbstractMCDTSOptimGoal, data::Dataset,\n                                        w::Int, delays, choose_mode; kwargs...)\n\nThis is one single rollout and backprop of the tree. For details please see\nthe accompanying paper [^Kraemer2021b].\n\n* `n`: Starting node\n* `optimalg::AbstractMCDTSOptimGoal`: Determines the delay preselection and\n  cost function (see [`MCDTSOptimGoal`](@ref)).\n* `data`: data\n* `w`: Theiler Window\n* `delays = 0:100`: The possible time lags\n* `choose_mode::Int=0`: Possibility for different modes of choosing the next\n  node based on which trial this is.\n\n## Keyword arguments\n* See [`mcdts_embedding`](@ref) for a list of all keywords.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.fnn_embedding_cycle","page":"Reference","title":"MCDTS.fnn_embedding_cycle","text":"fnn_embedding_cycle(NNdist, NNdistnew, r=2) -> FNNs\n\nCompute the amount of false nearest neighbors `FNNs`, when adding another component\nto a given (vector-) time series. This new component is the `τ`-lagged version\nof a univariate time series. `NNdist` is storing the distances of the nearest\nneighbor for all considered fiducial points and `NNdistnew` is storing the\ndistances of the nearest neighbor for each fiducial point in one embedding\ndimension higher using a given `τ`. The obligatory threshold `r` is by default\nset to 2.\n\n\n\n\n\n","category":"function"},{"location":"ref/#MCDTS.genembed_for_prediction-Union{Tuple{T}, Tuple{Vector{T}, Vector{Int64}}} where T","page":"Reference","title":"MCDTS.genembed_for_prediction","text":"genembed_for_prediction(Y, τs::Vector (,ts::Vector)) → Y_embed\n\nEmbeds the trajectory (or Vector) `Y` using the provided lags `τ`. If `Y` is a\ndataset, a vector `ts` must be provided, which stores the indices of the time\nseries contained in `Y`, which needs to be used for each embedding cycle. Here\nwe enforce a causal embedding, meaning that we shift the time series `Y` (or any\nof the time series in `Y`, if `Y` is a Dataset) by the negative values `τ` from\n`τs`.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.get_ar_prediction-Union{Tuple{T}, Tuple{Vector{T}, Vector{T} where T}} where T","page":"Reference","title":"MCDTS.get_ar_prediction","text":"get_ar_prediction(x::Vector, coeffs::Vector; kwargs...) → Y_predict\n\nComputes a prediction `Y_predict` of the AR-model determined by the coefficients\nin `coeffs`. The order of the AR-model equals the length of `coeffs`. `x` can be\na long vector (the time series), but needs to contain at least `length(coeffs)`\nvalues, in order to initialize the model. If the time horizon `Tw` is larger than\n1, an iterated one-step prediction is peformed.\n\nKeywords:\n* `Tw::Int = 1`: Time horizon for the prediction\n* `c::Real = 0`: Offset-parameter for AR-model\n* `rng::AbstractRNG = Random.GLOBAL_RNG`: Random number generator\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.get_binomial_table-Union{Tuple{T}, Tuple{T, T}} where T<:Real","page":"Reference","title":"MCDTS.get_binomial_table","text":"get_binomial_table(p, α; trial_range::Int=8) -> `δ_to_ε_amount`, Dict(δ_points => ϵ_points)\n\ncompute the numbers of points from the δ-neighborhood, which need to fall outside the ϵ-neighborhood, in order to reject the Null Hypothesis at a significance level α. One parameter of the binomial distribution is p, the other one would be the number of trials, i.e. the considered number of points of the δ-neighborhood. trial_range determines the number of considered δ-neighborhood-points, always starting from 8. For instance, if trial_range=8 (Default), then δ-neighborhood sizes from 8 up to 15 are considered. Return δ_to_ε_amount, a dictionary with δ_points as keys and the corresponding number of points in order to reject the Null, ϵ_points, constitute the values.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.get_delay_statistic-Tuple{MCDTS.Continuity_function, Any, Any, Any, Any, Any}","page":"Reference","title":"MCDTS.get_delay_statistic","text":"get_delay_statistic(optimalg.Λ<: AbstractDelayPreselection, Ys, τs, w, τ_vals, ts_vals; kwargs... )\n\nCompute the delay statistic according to the chosen method in `optimalg.Λ` (see [`MCDTSOptimGoal`](@ref))\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.get_embedding_params_according_to_loss-Tuple{MCDTS.AbstractLoss, Any, Any, Any, Any, Any}","page":"Reference","title":"MCDTS.get_embedding_params_according_to_loss","text":"get_embedding_params_according_to_loss(Γ::AbstractLoss, τ_pot, ts_popt, L_pot, L_old)\n\nHelper function for [`get_potential_delays`](@ref). Computes the potential\ndelay-, time series- and according Loss-values with respect to the actual loss\nin the current embedding cycle.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.get_max_idx-Union{Tuple{T}, Tuple{MCDTS.Range_function, Vector{T}, Any, Any, Any}} where T","page":"Reference","title":"MCDTS.get_max_idx","text":"get_max_idx(Λ::AbstractDelayPreselection, dps::Vector, τ_vals, ts_vals) → max_idx\n\nCompute the candidate delay values from the given delay pre-selection statistic\n`dps` with respect to `Λ`, which determined how `dps` was obtained and how\nto select the candidates (e.g. pick the maxima of `dps` in case of the\n`Λ` being the Continuity function). See [`Continuity_function`](@ref) and\n[`Range_function`](@ref).\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.get_maxima-Union{Tuple{Vector{T}}, Tuple{T}} where T","page":"Reference","title":"MCDTS.get_maxima","text":"Return the local maxima of the given time series s and its indices\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.get_potential_delays-Union{Tuple{T}, Tuple{D}, Tuple{MCDTS.AbstractMCDTSOptimGoal, DelayEmbeddings.Dataset{D, T}, Any, Int64, Any, Any, Any}} where {D, T}","page":"Reference","title":"MCDTS.get_potential_delays","text":"get_potential_delays(optimalg::AbstractMCDTSOptimGoal, Ys::Dataset, τs, w::Int, τ_vals,\n                ts_vals, L_old ; kwargs...]) → τ_pot, ts_pot, L_pot, flag\n\nCompute the potential delay `τ_pot` and time series values `ts_pot`, which would\neach result in a potential Loss-statistic value `L_pot`, by using an\nembedding method specified in `optimalg` [^Kraemer2021b] (see [`MCDTSOptimGoal`](@ref))\nand for a range of possible delay values `τs`. The input dataset `Ys` can be\nmultivariate. `w` is the Theiler window (neighbors in time with index `w` close\nto the point, that are excluded from being true neighbors. `w=0` means to\nexclude only the point itself, and no temporal neighbors. In case of multivariate\ntime series input choose `w` as the maximum of all `wᵢ's`. `τ_vals` and `ts_vals`\ndescribe the embedding up to the current embedding cycle.\n\n## Keyword arguments\n* See [`mcdts_embedding`](@ref) for a list of all keywords.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.integral_σ-Tuple{Any, Any}","page":"Reference","title":"MCDTS.integral_σ","text":"approximately calculate ∫_0 ^(ξ) σ(ζ) dζ.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.iterated_local_linear_prediction-Union{Tuple{DelayEmbeddings.Dataset{D, T}}, Tuple{T}, Tuple{D}, Tuple{DelayEmbeddings.Dataset{D, T}, Int64}, Tuple{DelayEmbeddings.Dataset{D, T}, Int64, Int64}} where {D, T}","page":"Reference","title":"MCDTS.iterated_local_linear_prediction","text":"iterated_local_linear_prediction(Y::Dataset, K::Int = 5, Tw::Int = 2; kwargs...) → Y_predict\n\nPerform an iterated one step forecast over `Tw` time steps using the local linear\nprediction algorithm. `Y_predict` is a Dataset of length `Tw` and dimension like\n`Y`.\n\nKeywords:\n* `metric = Euclidean()`: Metric used for distance computation\n* `theiler::Int = 1`: Theiler window for excluding serially correlated points from\n   the nearest neighbour search.\n* `verbose::Bool = false`: When set to `true`, the function prints the actual time\n  step, which it is computing.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.iterated_local_linear_prediction_embed-Union{Tuple{T}, Tuple{D}, Tuple{DelayEmbeddings.Dataset{D, T}, Vector{T} where T}, Tuple{DelayEmbeddings.Dataset{D, T}, Vector{T} where T, Int64}, Tuple{DelayEmbeddings.Dataset{D, T}, Vector{T} where T, Int64, Int64}} where {D, T}","page":"Reference","title":"MCDTS.iterated_local_linear_prediction_embed","text":"iterated_local_linear_prediction_embed(Y::Dataset, τs::Vector, K::Int = 5, Tw::Int = 2; kwargs...) → Y_predict\nPerform an iterated one step forecast over `Tw` time steps using the local linear\nprediction algorithm. `Y_predict` is a Dataset of length `Tw` and dimension like\n`Y`. In contrast to `iterated_local_linear_prediction()` we here use the time\ndelays `τs` to reconstruct all components of a predicted trajectory point from\nthe 1st component, which is obtained from the local model. This only works for\nunivariate embeddings.\n\nKeywords:\n* `metric = Euclidean()`: Metric used for distance computation\n* `theiler::Int = 1`: Theiler window for excluding serially correlated points from\n   the nearest neighbour search.\n* `verbose::Bool = false`: When set to `true`, the function prints the actual time\n  step, which it is computing.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.iterated_local_zeroth_prediction-Union{Tuple{DelayEmbeddings.Dataset{D, T}}, Tuple{T}, Tuple{D}, Tuple{DelayEmbeddings.Dataset{D, T}, Int64}, Tuple{DelayEmbeddings.Dataset{D, T}, Int64, Int64}} where {D, T}","page":"Reference","title":"MCDTS.iterated_local_zeroth_prediction","text":"iterated_local_zeroth_prediction(Y::Dataset, K::Int = 5, Tw::Int = 2; kwargs...) → Y_predict\n\nPerform an iterated one step forecast over Tw time steps using the local zeroth prediction algorithm. Y_predict is a Dataset of length Tw and dimension like Y.\n\nKeywords:\n\nmetric = Euclidean(): Metric used for distance computation\ntheiler::Int = 1: Theiler window for excluding serially correlated points from  the nearest neighbour search.\nverbose::Bool = false: When set to true, the function prints the actual time step, which it is computing.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.iterated_local_zeroth_prediction_embed-Union{Tuple{T}, Tuple{D}, Tuple{DelayEmbeddings.Dataset{D, T}, Vector{T} where T}, Tuple{DelayEmbeddings.Dataset{D, T}, Vector{T} where T, Int64}, Tuple{DelayEmbeddings.Dataset{D, T}, Vector{T} where T, Int64, Int64}} where {D, T}","page":"Reference","title":"MCDTS.iterated_local_zeroth_prediction_embed","text":"iterated_local_zeroth_prediction_embed(Y::Dataset, τs::Vector, K::Int = 5, Tw::Int = 2; kwargs...) → Y_predict\n\nPerform an iterated one step forecast over Tw time steps using the local linear prediction algorithm. Y_predict is a Dataset of length Tw and dimension like Y. In contrast to iterated_local_linear_prediction() we here use the time delays τs to reconstruct all components of a predicted trajectory point from the 1st component, which is obtained from the local model. This only works for univariate embeddings.\n\nKeywords:\n\nmetric = Euclidean(): Metric used for distance computation\ntheiler::Int = 1: Theiler window for excluding serially correlated points from  the nearest neighbour search.\nverbose::Bool = false: When set to true, the function prints the actual time step, which it is computing.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.jrp_rr_frac-Tuple{RecurrenceAnalysis.RecurrenceMatrix, RecurrenceAnalysis.RecurrenceMatrix}","page":"Reference","title":"MCDTS.jrp_rr_frac","text":"Computes the similarity between recurrence plots `RP₁` and `RP₂`. Outputs the\nfraction of recurrences rates gained from RP₁ and of the joint recurrence\nplot `RP₁ .* RP₂`.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.linear_prediction_cost-Union{Tuple{DelayEmbeddings.AbstractDataset{D, ET}}, Tuple{ET}, Tuple{D}} where {D, ET}","page":"Reference","title":"MCDTS.linear_prediction_cost","text":"linear_prediction_cost(Y::Dataset; kwargs...) → Cost\n\nCompute the mean squared one-step prediction error Cost of the Dataset Y. The prediction is based on local_linear_prediction.\n\nKeyword arguments\n\nsamplesize = 1.0: Number of considered fiducial points v as a fraction of input state space trajectory Y's length, in order to average the conditional variances and neighborhood sizes (read algorithm description) to produce L.\nK = 3: the amount of nearest neighbors considered, in order to compute the mean squared prediction error (read algorithm description).\nmetric = Euclidean(): metric used for finding nearest neigbhors in the input state space trajectory `Y.\nw = 1: Theiler window (neighbors in time with index w close to the point, that are excluded from being true neighbors). w=0 means to exclude only the point itself, and no temporal neighbors.\nTw = 1: The time horizon for predictions. The Cost is the average error over these timesteps.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.linear_prediction_cost_KL-Union{Tuple{DelayEmbeddings.AbstractDataset{D, ET}}, Tuple{ET}, Tuple{D}} where {D, ET}","page":"Reference","title":"MCDTS.linear_prediction_cost_KL","text":"linear_prediction_cost_KL(Y::Dataset; kwargs...) → Cost\n\nCompute the KL-divergence Cost of the Dataset Y. The prediction is based on local_linear_prediction.\n\nKeyword arguments\n\nsamplesize = 1.0: Number of considered fiducial points v as a fraction of input state space trajectory Y's length, in order to average the conditional variances and neighborhood sizes (read algorithm description) to produce L.\nK = 3: the amount of nearest neighbors considered, in order to compute the mean squared prediction error (read algorithm description).\nmetric = Euclidean(): metric used for finding nearest neigbhors in the input state space trajectory `Y.\nw = 1: Theiler window (neighbors in time with index w close to the point, that are excluded from being true neighbors). w=0 means to exclude only the point itself, and no temporal neighbors.\nTw = 1: The time horizon for predictions. The Cost is the average error over these timesteps.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.local_linear_prediction-Union{Tuple{DelayEmbeddings.Dataset{D, T}}, Tuple{T}, Tuple{D}, Tuple{DelayEmbeddings.Dataset{D, T}, Int64}} where {D, T}","page":"Reference","title":"MCDTS.local_linear_prediction","text":"local_linear_prediction(Y::Dataset, K::Int = 5; kwargs...) → x_pred, e_expect\n\nPerform a prediction for the time horizon `Tw` (default = 1) by a locally linear\nfit. Based on `K` nearest neighbours of the last point of the given trajectory\n`Y`, we fit a linear model to these points and their `Tw`-step ahead images. The\noutput `x_pred` is, thus, the `Tw`-step ahead prediction vector.\nThe function also returns `e_expect`, the expected error on the prediction `x_pred`,\ncomputed as the mean of the RMS-errors of all `K`-neighbours-errors.\n\nKeywords:\n* `metric = Euclidean()`: Metric used for distance computation\n* `theiler::Int = 1`: Theiler window for excluding serially correlated points from\n   the nearest neighbour search.\n* `Tw::Int = 1`: The prediction time in sampling units. If `Tw > 1`, a multi-step\n  prediction is performed.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.local_random_analogue_prediction-Union{Tuple{DelayEmbeddings.Dataset{D, T}}, Tuple{T}, Tuple{D}, Tuple{DelayEmbeddings.Dataset{D, T}, Int64}} where {D, T}","page":"Reference","title":"MCDTS.local_random_analogue_prediction","text":"local_random_analogue_prediction(Y::Dataset, K::Int; kwargs...) → Y_predict\n\nCompute a one step ahead prediction Y_predict of the input Y, based on K nearest neighbors. Here the prediction is a random pick from all K-nearest neighbour images and, thus, invokes some kind of randomness.\n\nKeywords:\n\nmetric = Euclidean(): Metric used for distance computation\ntheiler::Int = 1: Theiler window for excluding serially correlated points from  the nearest neighbour search.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.local_zeroth_prediction-Union{Tuple{DelayEmbeddings.Dataset{D, T}}, Tuple{T}, Tuple{D}, Tuple{DelayEmbeddings.Dataset{D, T}, Int64}} where {D, T}","page":"Reference","title":"MCDTS.local_zeroth_prediction","text":"local_zeroth_prediction(Y::Dataset, K::Int = 5; kwargs...) → x_pred, e_expect\n\nPerform a \"zeroth\" order prediction for the time horizon Tw (default = 1). Based on K nearest neighbours of the last point of the given trajectory Y, the Tw-step ahead prediction is simply the mean of the images of these K-nearest neighbours. The output x_pred is, thus, the Tw-step ahead prediction vector. The function also returns e_expect, the expected error on the prediction x_pred, computed as the mean of the RMS-errors of all K-neighbours-errors.\n\nKeywords:\n\nmetric = Euclidean(): Metric used for distance computation\ntheiler::Int = 1: Theiler window for excluding serially correlated points from  the nearest neighbour search.\nTw::Int = 1: The prediction time in sampling units. If Tw > 1, a multi-step prediction is performed.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.make_prediction-Union{Tuple{ET}, Tuple{D}, Tuple{MCDTS.AbstractPredictionMethod{:zeroth}, DelayEmbeddings.AbstractDataset{D, ET}}} where {D, ET}","page":"Reference","title":"MCDTS.make_prediction","text":"make_prediction(pred_meth::AbstractPredictionMethod, Y::AbstractDataset{D, ET};\n        K::Int = 3, w::Int = 1, Tw::Int = 1, metric = Euclidean()) → prediction\n\nCompute a in-sample `Tw`-time-steps-ahead of the data `Y`, using the prediction\nmethod `pred_meth`. `w` is the Theiler window and `K` the nearest neighbors used.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.mc_delay-Tuple","page":"Reference","title":"MCDTS.mc_delay","text":"Legacy name, please use [`mcdts_embedding`](@ref)\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.mcdts_embedding-Union{Tuple{D}, Tuple{DelayEmbeddings.Dataset, MCDTS.AbstractMCDTSOptimGoal, Int64, AbstractRange{D}}, Tuple{DelayEmbeddings.Dataset, MCDTS.AbstractMCDTSOptimGoal, Int64, AbstractRange{D}, Int64}} where D","page":"Reference","title":"MCDTS.mcdts_embedding","text":"mcdts_embedding\n\n## Convenience / default option call\n\nmcdts_embedding(data::Dataset, N::Int=100; kwargs...)\n\nDo the MCDTS embedding of the `data` with `N` trials according to the\nPECUZAL algorithm [^Kraemer2021], returns the tree. Embedding parameters\nlike the Theiler window, considered delays and the function that chooses the\nnext embedding step are all estimated automatically or the default option is\nused. `data` is a `DynamicalSystems.Dataset`.\n\n## All options\n\nmcdts_embedding(data::DynamicalSystems.Dataset, optimalg::AbstractMCDTSOptimGoal,\n                        w::Int, delays::AbstractRange{D}, N::Int=40;  kwargs...)\n\n* `optimalg::AbstractMCDTSOptimGoal` determines how the embedding is performed in\n  each cycle. Specifically it sets the delay pre-selection statistic Λ, which\n  pre-selects the potential delays in each embedding cycle as well as the the\n  Loss-Statistic Γ, which determines the Loss to be minimized by MCDTS\n  [^Kraemer2021b] (see [`MCDTSOptimGoal`](@ref)).\n* `w` is the Theiler window (neighbors in time with index `w` close to the point,\n  that are excluded from being true neighbors. `w=0` means to exclude only the\n  point itself, and no temporal neighbors. In case of multivariate time series\n  input choose `w` as the maximum of all `wᵢ's`. As a default in the convience\n  call this is estimated with a mutual information minimum method of DelayEmbeddings.jl\n* `delays = 0:100`: The possible time lags\n* `N::Int`: The number of tree expansions\n\n## Keyword Arguments\n* `choose_func`: Function to choose next node in the tree with, default\n  choice: `(L)->(MCDTS.softmaxL(L,β=2.))`\n* `max_depth = 20`: Threshold, which determines the algorithm. It either breaks,\n  when it converges, i.e. when there is no way to reduce the cost-function any\n  further, or when this threshold is reached.\n* `verbose`: Either `true` or `false` (default); prints status of embedding optimization.\n* `metric`: norm for distance computation (default is `Euclidean()`)\n\n\n\n\n\n\n* `KNN = 3`: The number of nearest neighbors considered in the computation of\n  the L-statistic.\n* `FNN:Bool = false`: Determines whether the algorithm should minimize the\n  L-statistic or the FNN-statistic.\n* `PRED::Bool = false`: Determines whether the algorithm should minimize the\n  L-statistic or a cost function based on minimizing the `Tw`-step-prediction error\n* `Tw::Int = 1`: If `PRED = true`, this is the considered prediction horizon\n* `linear::Bool=false`: If `PRED = true`, this determines whether the prediction shall\n  be made on the zeroth or a linear predictor.\n* `PRED_mean::Bool=false`: If `PRED = true`, this determines whether the prediction shall\n  be optimized on the mean MSE of all components or only on the 1st-component (Default)\n* `PRED_L::Bool=false`: If `PRED = true`, this determines whether the prediction shall\n  be optimized on possible delay values gained from the continuity statistic or on\n  delays = 0:25 (Default)\n* `PRED_KL::Bool=false`: If `PRED = true`, this determines whether the prediction shall\n  be optimized on the Kullback-Leibler-divergence of the in-sample prediction and\n  the true in-sample values, or if the optimization shall be made on the MSE of them (Default)\n* `CCM:Bool=false`: Determines whether the algorithm should maximize the CCM-correlation\n  coefficient of the embedded vector from `Ys` and the given time series `Y_CCM`.\n* `Y_CCM`: The time series CCM should cross map to.\n* `threshold::Real = 0`: The algorithm does not pick a peak from the continuity\n  statistic, when its corresponding `ΔL`/FNN-value exceeds this threshold. Please\n  provide a positive number for both, `L` and `FNN`-statistic option (since the\n  `ΔL`-values are negative numbers for meaningful embedding cycles, this threshold\n  gets internally sign-switched).\n* `tws::Range = 2:delays[end]`: Customization of the sampling of the different T's,\n  when computing Uzal's L-statistics. Here any kind of integer ranges (starting\n  at 2) are allowed, up to `delays[end]`.\n\n  [^Kraemer2021]: Kraemer, K.H., Datseris, G., Kurths, J., Kiss, I.Z., Ocampo-Espindola, Marwan, N. (2021). [A unified and automated approach to attractor reconstruction. New Journal of Physics 23(3), 033017](https://iopscience.iop.org/article/10.1088/1367-2630/abe336).\n  [^Kraemer2021b]: Kraemer, K.H., Gelbrecht, M., Pavithran, I., Sujith, R. I. and Marwan, N. (2021). [Optimal state space reconstruction via Monte Carlo Decision Tree Search. Submitted to Nonlinear Dynamics](https://doi.org/10.21203/rs.3.rs-899760/v1)\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.mfnn-Tuple{DelayEmbeddings.Dataset, DelayEmbeddings.Dataset}","page":"Reference","title":"MCDTS.mfnn","text":"Computes the mututal false nearest neighbours (mfnn) for a reference trajectory\n`Y_ref` and a reconstruction `Y_rec` after [^Rulkov1995].\n\n## Keyword arguments\n*`w = 1`: Theiler window for the surpression of serially correlated neighbors in\n    the nearest neighbor-search\n*`kNN = 1`: The number of considered nearest neighbours (in the paper always 1)\n\n[^Rulkov1995]: Rulkov, Nikolai F. and Sushchik, Mikhail M. and Tsimring, Lev S. and Abarbanel, Henry D.I. (1995). [Generalized synchronization of chaos in directionally coupled chaotic systems. Physical Review E 51, 980](https://doi.org/10.1103/PhysRevE.51.980).\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.moving_average-Tuple{Any, Any}","page":"Reference","title":"MCDTS.moving_average","text":"Moving average of a timeseries `vs` over a window `n`\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.next_embedding-Union{Tuple{T}, Tuple{D}, Tuple{MCDTS.Node, MCDTS.AbstractMCDTSOptimGoal, DelayEmbeddings.Dataset{D, T}, Int64, Any}} where {D, T<:Real}","page":"Reference","title":"MCDTS.next_embedding","text":"next_embedding(n::Node, optimalg::AbstractMCDTSOptimGoal, Ys::Dataset{D, T},\n                        w::Int, τs; kwargs...) → τ_pot, ts_pot, L_pot, flag\n\nPerforms the next embedding step. For the actual embedding contained in tree\nleaf `n` compute as many statistics determined by `optimalg.Λ`\n(see [`MCDTSOptimGoal`](@ref)) as there are time series in the Dataset\n`Ys` for a range of possible delays `τs`. Return the values for the best delay\n`τ_pot`, its corresponding time series index `ts_pot` the according Loss-value\n`L_pot` and `flag`, following the minimization of the Loss determined by\n`optimalg.Γ`.\n\n## Keyword arguments\n* See [`mcdts_embedding`](@ref) for a list of all keywords.\n\n## Returns\n* `τ_pot`: Next delay\n* `ts_pot`: Index of the time series used (in case of multivariate time series)\n* `L_pot`: L statistic of next embedding step with delay `τ_pot` from `ts_pot`.\n* `flag`: Did the embedding converge? i.e. L can not be further minimized anymore\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.nonlin_noise_reduction-Union{Tuple{T}, Tuple{Vector{T}, Int64, Int64}} where T","page":"Reference","title":"MCDTS.nonlin_noise_reduction","text":"Simple nonlinear noise reduction algorithm by Schreiber 1993\n\nParams:\n*`m`           denotes the local embedding dimension\n*`epsilon`     denotes the local neighborhood size as number of neighbours\n\nReturns:\nthe filtered signal\n\nNote that by applying this filter, there will be lost `m-1` datapoints.\nWe therefore phase-shift each datapoint in the resulting signal by `(m-1)/2`.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.pecora-Union{Tuple{T}, Tuple{D}, Tuple{Any, Tuple{Vararg{Int64, D}}}, Tuple{Any, Tuple{Vararg{Int64, D}}, Tuple{Vararg{Int64, D}}}} where {D, T<:Real}","page":"Reference","title":"MCDTS.pecora","text":"pecora(s, τs, js; kwargs...) → ⟨ε★⟩, ⟨Γ⟩\n\nCompute the (average) continuity statistic ⟨ε★⟩ and undersampling statistic ⟨Γ⟩ according to Pecora et al.[Pecoral2007] (A unified approach to attractor reconstruction), for a given input s (timeseries or Dataset) and input generalized embedding defined by (τs, js), according to genembed. The continuity statistic represents functional independence between the components of the existing embedding and one additional timeseries. The returned results are matrices with size TxJ.\n\nKeyword arguments\n\ndelays = 0:50: Possible time delay values delays (in sampling time units). For each of the τ's in delays the continuity-statistic ⟨ε★⟩ gets computed. If undersampling = true (see further down), also the undersampling statistic ⟨Γ⟩ gets returned for all considered delay values.\nJ = 1:dimension(s): calculate for all timeseries indices in J. If input s is a timeseries, this is always just 1.\nsamplesize::Real = 0.1: determine the fraction of all phase space points (=length(s)) to be considered (fiducial points v) to average ε★ to produce ⟨ε★⟩, ⟨Γ⟩\nK::Int = 13: the amount of nearest neighbors in the δ-ball (read algorithm description). Must be at least 8 (in order to gurantee a valid statistic). ⟨ε★⟩ is computed taking the minimum result over all k ∈ K.\nmetric = Chebyshev(): metrix with which to find nearest neigbhors in the input embedding (ℝᵈ space, d = length(τs)).\nw = 1: Theiler window (neighbors in time with index w close to the point, that are excluded from being true neighbors). w=0 means to exclude only the point itself, and no temporal neighbors.\nundersampling = false : whether to calculate the undersampling statistic or not (if not, zeros are returned for ⟨Γ⟩). Calculating ⟨Γ⟩ is thousands of times slower than ⟨ε★⟩.\ndb::Int = 100: Amount of bins used into calculating the histograms of each timeseries (for the undersampling statistic).\nα::Real = 0.05: The significance level for obtaining the continuity statistic\np::Real = 0.5: The p-parameter for the binomial distribution used for the computation of the continuity statistic.\n\nDescription\n\nNotice that the full algorithm is too large to discuss here, and is written in detail (several pages!) in the source code of pecora.\n\n[Pecora2007]: Pecora, L. M., Moniz, L., Nichols, J., & Carroll, T. L. (2007). A unified approach to attractor reconstruction. Chaos 17(1).\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.perform_recurrence_analysis-NTuple{4, DelayEmbeddings.Dataset}","page":"Reference","title":"MCDTS.perform_recurrence_analysis","text":"Perform the RecurrenceAnalysis of some reconstruction trajectories `Y₁`, `Y₂`,\n`Y₃`. Specifically, compute the fraction of recurrence rates from the\n\"original\"/reference trajectory `Y_ref` with the one from the JRP of the\noriginal `Y₁`, `Y₂`, `Y₃` together with the reconstructed trajectory. Also\ncompute RQA quantifiers of the recurrence plots of `Y₁`, `Y₂`, `Y₃` and `Y_ref`.\n\n## Keyword arguments:\n*`ε = 0.05`: The used threshold for constructing the recurrence plots\n    The reconstruction method is fixed recurrence rate.\n*`w = 1`: Theiler window used for all Datasets\n*`lmin = 2`: Minimum used line length for digaonal line based RQA measures.\n*`kNN = 1`: The number of nearest neighbors used for obtaining the mutual\n    nearest neighbors measure\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.pick_possible_embedding_params-Union{Tuple{T}, Tuple{D}, Tuple{MCDTS.AbstractLoss, MCDTS.AbstractDelayPreselection, Any, DelayEmbeddings.Dataset{D, T}, Any, Any, Int64, Any, Any}} where {D, T}","page":"Reference","title":"MCDTS.pick_possible_embedding_params","text":"Compute all possible τ-values (and according time series numbers) and their\ncorresponding Loss-statistics for the input delay_pre_selection_statistic `dps`.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.rw_norm-Union{Tuple{T}, Tuple{Vector{T}, Int64}} where T","page":"Reference","title":"MCDTS.rw_norm","text":"Compute the scaling term for the MASE measure. This is tge average in-sample\nforecast error for a random-walk prediction, which uses the previous value in\nthe observed signal `x` as the forecast. `Tw` is the prediction time horizon.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.softmaxL-Tuple{Any}","page":"Reference","title":"MCDTS.softmaxL","text":"softmaxL(Ls; β=1.5)\n\nReturns an index with prob computed by a softmax of all Ls.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.uzal_cost_pecuzal_mcdts-Union{Tuple{ET}, Tuple{DT}, Tuple{D}, Tuple{DelayEmbeddings.Dataset{D, ET}, DelayEmbeddings.Dataset{DT, ET}, Int64}} where {D, DT, ET}","page":"Reference","title":"MCDTS.uzal_cost_pecuzal_mcdts","text":"uzal_cost_pecuzal_mcdts(Y1::Dataset, Y2::Dataset, Tw; kwargs...) → L_decrease\n\nThis function is based on the functionality of [`uzal_cost`](@ref), here\nspecifically tailored for the needs in the PECUZAL algorithm.\nCompute the L-statistics `L1` and `L2` for the input datasets `Y1` and `Y2` for\nincreasing time horizons `T = 1:Tw`. For each `T`, compute `L1` and `L2` and\ndecrease `L_decrease = L2 - L1`. If `L_decrease` is a negative value, then `Y2`\ncan be regarded as a \"better\" reconstruction that `Y1`. Break, when `L_decrease`\nreaches the 1st local minima, since this will typically also be the global\nminimum. Return the according minimum `L_decrease`-value.\n\n## Keyword arguments\n\n* `K = 3`: the amount of nearest neighbors considered, in order to compute σ_k^2\n  (read algorithm description).\n  If given a vector, minimum result over all `k ∈ K` is returned.\n* `metric = Euclidean()`: metric used for finding nearest neigbhors in the input\n  state space trajectory `Y.\n* `w = 1`: Theiler window (neighbors in time with index `w` close to the point,\n  that are excluded from being true neighbors). `w=0` means to exclude only the\n  point itself, and no temporal neighbors.\n* `econ::Bool = false`: Economy-mode for L-statistic computation. Instead of\n  computing L-statistics for time horizons `2:Tw`, here we only compute them for\n  `2:2:Tw`.\n* `tws::Range = 2:Tw`: Further customization of the sampling of the different T's.\n  While `econ=true` gives `tws = 2:2:Tw`, here any kind of interger ranges (starting at 2)\n  are allowed, up to `Tw`.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.zeroth_prediction_cost-Union{Tuple{DelayEmbeddings.AbstractDataset{D, ET}}, Tuple{ET}, Tuple{D}} where {D, ET}","page":"Reference","title":"MCDTS.zeroth_prediction_cost","text":"zeroth_prediction_cost(Y::Dataset; kwargs...) → Cost\n\nCompute the mean squared one-step prediction error `Cost` of the Dataset `Y`.\nThe prediction is based on [`local_zeroth_prediction`](@ref).\n\n## Keyword arguments\n\n* `samplesize = 1.0`: Number of considered fiducial points v as a fraction of\n  input state space trajectory `Y`'s length, in order to average the conditional\n  variances and neighborhood sizes (read algorithm description) to produce `L`.\n* `K = 3`: the amount of nearest neighbors considered, in order to compute the\n  mean squared prediction error (read algorithm description).\n* `metric = Euclidean()`: metric used for finding nearest neigbhors in the input\n  state space trajectory `Y.\n* `w = 1`: Theiler window (neighbors in time with index `w` close to the point,\n  that are excluded from being true neighbors). `w=0` means to exclude only the\n  point itself, and no temporal neighbors.\n* `Tw = 1`: The time horizon for predictions. The `Cost` is the average error\n  over these timesteps.\n\n\n\n\n\n","category":"method"},{"location":"ref/#MCDTS.zeroth_prediction_cost_KL-Union{Tuple{DelayEmbeddings.AbstractDataset{D, ET}}, Tuple{ET}, Tuple{D}} where {D, ET}","page":"Reference","title":"MCDTS.zeroth_prediction_cost_KL","text":"zeroth_prediction_cost_KL(Y::Dataset; kwargs...) → Cost\n\nCompute the KL-divergence Cost of the Dataset Y. The prediction is based on local_zeroth_prediction.\n\nKeyword arguments\n\nsamplesize = 1.0: Number of considered fiducial points v as a fraction of input state space trajectory Y's length, in order to average the conditional variances and neighborhood sizes (read algorithm description) to produce L.\nK = 3: the amount of nearest neighbors considered, in order to compute the mean squared prediction error (read algorithm description).\nmetric = Euclidean(): metric used for finding nearest neigbhors in the input state space trajectory `Y.\nw = 1: Theiler window (neighbors in time with index w close to the point, that are excluded from being true neighbors). w=0 means to exclude only the point itself, and no temporal neighbors.\nTw = 1: The time horizon for predictions. The Cost is the average error over these timesteps.\n\n\n\n\n\n","category":"method"},{"location":"#Monte-Carlo-Decision-Tree-Search-for-optimal-embedding","page":"Home","title":"Monte Carlo Decision Tree Search for optimal embedding","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This project implements the MCDTS algorithm outlined in the paper ____. It aims to provide an optimal time delay state space reconstruction from time series data with the help of decisions trees and suitable statistics that guide the decisions done during the rollout of these trees. For all details of the algorithm the reader is referred to the accompanying paper. Here we provide an implementation of all the variants described in the paper. All major functions have docstrings that describe their use. In what follows basic use examples are outlined.","category":"page"},{"location":"#Usage","page":"Home","title":"Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"As a first example, we embedd a Lorenz63 system. This is meant as a toy example: we generate data from a Lorenz63 system and then try to reconstructe the full state space from only one of the three observables.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For this we also make use of DynamicalSystems.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"First we import the needed modules and generate a long trajectory of the Lorenz system (and discard any transient dynamics)","category":"page"},{"location":"","page":"Home","title":"Home","text":"using DynamicalSystems, MCDTS, Random, Test, DelayEmbeddings\n\n# Check Lorenz System\nRandom.seed!(1234)\nds = Systems.lorenz()\ndata = trajectory(ds,200)\ndata = data[10001:end,:]","category":"page"},{"location":"","page":"Home","title":"Home","text":"The easiest way to use MCDTS for embedding is to use it with its default options just as","category":"page"},{"location":"","page":"Home","title":"Home","text":"tree = mcdts_embedding(data, 100)","category":"page"},{"location":"","page":"Home","title":"Home","text":"here with N=100 trials. This will perform the MCDTS algorithm and return the full decision tree. The best embedding can then just be printed as","category":"page"},{"location":"","page":"Home","title":"Home","text":"best_node = MCDTS.best_embedding(tree)\nprintln(best_node)","category":"page"},{"location":"","page":"Home","title":"Home","text":"e.g.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Node with τ=12, i_t=1 ,L=-1.5795030971438209 - full embd. τ=[0, 61, 48, 12] ,i_ts=[1, 1, 1, 1]","category":"page"},{"location":"","page":"Home","title":"Home","text":"This version of mcdts_embedding uses default options to estimate all parameters of the algorithm. It is of course also possible to choose these individually and thus also use all different versions that are presented in the paper.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The most important paremters are the Theiler window, the minimum temporal distance for points that are considered neighours in phase space. We can get an estimate with the help of DelayEmbeddings.jl (part of DynamicalSystems.jl)","category":"page"},{"location":"","page":"Home","title":"Home","text":"w1 = DelayEmbeddings.estimate_delay(data[:,1],\"mi_min\")\nw2 = DelayEmbeddings.estimate_delay(data[:,2],\"mi_min\")\nw3 = DelayEmbeddings.estimate_delay(data[:,3],\"mi_min\")\nw = maximum(hcat(w1,w2,w3))\ndelays = 0:100\nN_trials = 100","category":"page"},{"location":"","page":"Home","title":"Home","text":"Also, we set the range of delays that we want to consider, here 0:100. Next, we speficy the wanted optimization. The default optimziation goal is the Pecuzal algorithm, as outlined in the paper, with","category":"page"},{"location":"","page":"Home","title":"Home","text":"pecuzal = MCDTS.PecuzalOptim()","category":"page"},{"location":"","page":"Home","title":"Home","text":"Then we can call the proper mcdts_embedding function with","category":"page"},{"location":"","page":"Home","title":"Home","text":"MCDTS.mcdts_embedding(Dataset(data[:,1]), pecuzal, w1, delays, runs)\nbest_node = MCDTS.best_embedding(tree)\nprintln(best_node)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Further optimisation goals can be set by combining an AbstractDelayPreselection with an AbstractLoss, e.g. via","category":"page"},{"location":"","page":"Home","title":"Home","text":"optimgoal = MCDTS.MCDTSOptimGoal(MCDTS.FNN_statistic(0.05), MCDTS.Continuity_function())","category":"page"},{"location":"","page":"Home","title":"Home","text":"which uses the FNN statistic with a threshold of 0.05 as a loss function and the continuity as a delay preselection method. ","category":"page"}]
}
